{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch==2.3.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch==2.3.0) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch==2.3.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch==2.3.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch==2.3.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch==2.3.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch==2.3.0) (2023.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from jinja2->torch==2.3.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from sympy->torch==2.3.0) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchtext) (4.66.1)\n",
      "Requirement already satisfied: requests in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch>=2.3.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchtext) (2.3.0)\n",
      "Requirement already satisfied: numpy in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchtext) (1.24.3)\n",
      "Requirement already satisfied: filelock in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2.3.0->torchtext) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2.3.0->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2.3.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2.3.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2.3.0->torchtext) (2023.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchtext) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchtext) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: portalocker in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (2.10.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchdata in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (0.8.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchdata) (1.26.16)\n",
      "Requirement already satisfied: requests in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: torch>=2 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torchdata) (2.3.0)\n",
      "Requirement already satisfied: filelock in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2->torchdata) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2->torchdata) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2->torchdata) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2->torchdata) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2->torchdata) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from torch>=2->torchdata) (2023.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchdata) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from requests->torchdata) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=2->torchdata) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/ishanshastri/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.0 # To work with torchtext\n",
    "!pip install torchtext\n",
    "!pip install portalocker\n",
    "!pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from typing import Iterable, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import einops\n",
    "from tokenizers import CharBPETokenizer\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import huggingface char-bpe tokenizer\n",
    "en_tokenizer, de_tokenizer = CharBPETokenizer(), CharBPETokenizer()\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "en_tokenizer.add_special_tokens(special_symbols)\n",
    "de_tokenizer.add_special_tokens(special_symbols)\n",
    "\n",
    "# Train tokenizers\n",
    "train_iter, test_iter, valid_iter = Multi30k(language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "\n",
    "de_data, en_data = (list(zip(*train_iter)))\n",
    "de_data_test, en_data_test = (list(zip(*test_iter)))\n",
    "\n",
    "en_tokenizer.train_from_iterator(iterator=en_data)\n",
    "de_tokenizer.train_from_iterator(iterator=de_data)\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = tuple(en_tokenizer.encode(x).ids[0] for x in special_symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Two</w>',\n",
       "  'young</w>',\n",
       "  ',</w>',\n",
       "  'White</w>',\n",
       "  'males</w>',\n",
       "  'are</w>',\n",
       "  'outside</w>',\n",
       "  'near</w>',\n",
       "  'many</w>',\n",
       "  'bushes</w>',\n",
       "  '.</w>'],\n",
       " [218, 255, 112, 2727, 1295, 201, 375, 463, 1086, 2994, 154])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.encode(en_data[0]).tokens, en_tokenizer.encode(en_data[0]).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text transforms for data\n",
    "text_transform_en = lambda x : torch.tensor([BOS_IDX] + en_tokenizer.encode(x.rstrip(\"\\n\")).ids + [EOS_IDX])\n",
    "text_transform_de = lambda x : torch.tensor([BOS_IDX] + de_tokenizer.encode(x.rstrip(\"\\n\")).ids + [EOS_IDX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Position encoding\"\"\"\n",
    "class PositionEncoding2(nn.Module):\n",
    "    def __init__(self, d_embed):\n",
    "        super(PositionEncoding2, self).__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def get_position_encoding(self, seq_len):\n",
    "        encoding = torch.zeros((seq_len, self.d_embed))\n",
    "        dimensions = torch.arange(0, self.d_embed//2)\n",
    "        timesteps = torch.arange(0, seq_len)\n",
    "\n",
    "        encoding[:, 0::2] = torch.sin(torch.einsum('i,j -> ji', 1/(10000**(2*dimensions/self.d_embed)), timesteps))\n",
    "        encoding[:, 1::2] = torch.cos(torch.einsum('i,j -> ji', 1/(10000**(2*dimensions/self.d_embed)), timesteps))\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        pos_encoding = nn.Parameter(self.get_position_encoding(seq_len=inp.shape[-2]), requires_grad=False).to(DEVICE)\n",
    "        return self.dropout(inp + pos_encoding) # + dropout ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multihead attention\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, n_heads):\n",
    "        super().__init__()#MultiheadAttention)\n",
    "        self.d_hidden=d_hidden\n",
    "        self.n_heads = n_heads\n",
    "        self.W_q = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden//n_heads)))\n",
    "        )\n",
    "        self.W_k = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden//n_heads)))\n",
    "        )\n",
    "        self.W_v = nn.Parameter(\n",
    "            nn.init.xavier_normal_(torch.zeros((n_heads, d_model, d_hidden//n_heads)))\n",
    "        )\n",
    "\n",
    "        self.W_o = nn.Linear(in_features=d_hidden, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v, get_attn_scores=False, mask=None, is_causal=False):\n",
    "        mask_shape = (q.shape[-2], k.shape[-2]) \n",
    "        # print(\"mask shape: \", mask_shape)\n",
    "        mask = mask if mask else (self.causal_mask(self.n_heads, mask_shape) if is_causal else self.empty_mask(self.n_heads, mask_shape))\n",
    "        mask = mask.to(DEVICE)\n",
    "        \n",
    "        # Compute Q, K, V (matrix multiplication, batched along heads)\n",
    "        q = einops.einsum(self.W_q, q, 'h d_model d_h, b T d_model -> b h  T d_h') # T is timesteps (sequence length)\n",
    "        k = einops.einsum(self.W_k, k, 'h d_model d_h, b T d_model -> b h  T d_h')\n",
    "        v = einops.einsum(self.W_v, v, 'h d_model d_h, b T d_model -> b h  T d_h')\n",
    "\n",
    "        # print(\"q_shape: \", q.shape)\n",
    "        # print(\"k_shape: \", k.shape)\n",
    "\n",
    "        attn_scores = torch.softmax(\n",
    "            einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(self.d_hidden//self.n_heads) + mask,#torch.FloatTensor(self.d_hidden//8)),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        attn_head_outs = einops.einsum(attn_scores, v, '... T_out T_in, ... T_in d_h -> ... T_out d_h')\n",
    "\n",
    "        concatted_outs = einops.rearrange(attn_head_outs, 'b h T_out d_h -> b T_out (h d_h)')\n",
    "        concatted_outs = self.W_o(concatted_outs)\n",
    "\n",
    "        return (concatted_outs, attn_scores) if get_attn_scores else concatted_outs\n",
    "    \n",
    "    @classmethod\n",
    "    def causal_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.triu(\n",
    "                torch.fill(torch.zeros(mask_shape),  -torch.inf),\n",
    "                diagonal=1\n",
    "                )\n",
    "            , pattern='... -> k ...', k=n_heads\n",
    "            )\n",
    "    \n",
    "    @classmethod\n",
    "    def empty_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.zeros(mask_shape) , pattern='... -> k ...', k=n_heads\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multihead attention\"\"\"\n",
    "class MultiHeadAttention2(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, n_heads):\n",
    "        super().__init__()#MultiheadAttention)\n",
    "        self.d_hidden=d_hidden\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_hidden)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_hidden)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_hidden)\n",
    "\n",
    "        self.W_o = nn.Linear(in_features=d_hidden, out_features=d_model, bias=False)\n",
    "\n",
    "    def forward(self, q, k, v, get_attn_scores=False, mask=None, is_causal=False):\n",
    "        mask_shape = (q.shape[-2], k.shape[-2]) \n",
    "        # print(\"mask shape: \", mask_shape)\n",
    "        mask = mask if mask else (self.causal_mask(self.n_heads, mask_shape) if is_causal else self.empty_mask(self.n_heads, mask_shape))\n",
    "        mask = mask.to(DEVICE)\n",
    "\n",
    "        # Compute Q, K, V (matrix multiplication, batched along heads)\n",
    "        q = self.W_q(q)\n",
    "        k = self.W_k(k)\n",
    "        v = self.W_v(v)\n",
    "\n",
    "        # Split into attention heads\n",
    "        q = einops.rearrange(q, 'b T (h w) -> b h T w', h=self.n_heads)\n",
    "        k = einops.rearrange(k, 'b T (h w) -> b h T w', h=self.n_heads)\n",
    "        v = einops.rearrange(v, 'b T (h w) -> b h T w', h=self.n_heads)\n",
    "\n",
    "        attn_scores = torch.softmax(\n",
    "            einops.einsum(q, k, 'b h T_out d_h, b h T_in d_h -> b h T_out T_in')/math.sqrt(self.d_hidden//self.n_heads) + mask,#torch.FloatTensor(self.d_hidden//8)),\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        attn_head_outs = einops.einsum(attn_scores, v, '... T_out T_in, ... T_in d_h -> ... T_out d_h')\n",
    "\n",
    "        concatted_outs = einops.rearrange(attn_head_outs, 'b h T_out d_h -> b T_out (h d_h)')\n",
    "        concatted_outs = self.W_o(concatted_outs)\n",
    "\n",
    "        return (concatted_outs, attn_scores) if get_attn_scores else concatted_outs\n",
    "    \n",
    "    @classmethod\n",
    "    def causal_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.triu(\n",
    "                torch.fill(torch.zeros(mask_shape),  -torch.inf),\n",
    "                diagonal=1\n",
    "                )\n",
    "            , pattern='... -> k ...', k=n_heads\n",
    "            )\n",
    "    \n",
    "    @classmethod\n",
    "    def empty_mask(cls, n_heads, mask_shape):\n",
    "        return einops.repeat(\n",
    "            torch.zeros(mask_shape) , pattern='... -> k ...', k=n_heads\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, d_ff=2048):\n",
    "        super(PosWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # FFN(x) = max(0, xW1 + b1)W2 + b2 ; f(x) = max(0, x) is relu\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, num_heads, d_ff, dropout=0.1, N_layers=6, model_kind=1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_hidden, num_heads) if model_kind==1 else MultiHeadAttention2(d_model, d_hidden, num_heads)\n",
    "        self.feed_forward = PosWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None, is_causal=False):\n",
    "        #print(\"Q, K, V: \", x.size())\n",
    "        attn_output = self.self_attn(x, x, x, mask=mask, is_causal=is_causal) # self attention\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # layer-norm + dropout + skip connection\n",
    "        ff_output = self.feed_forward(x) # feed-forward\n",
    "        x = self.norm2(x + self.dropout(ff_output)) # layer-norm + dropout + skip connection\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, num_heads, d_ff, dropout=0.1, N_layers=6, model_kind=1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, d_hidden, num_heads) if model_kind==1 else MultiHeadAttention2(d_model, d_hidden, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, d_hidden, num_heads) if model_kind==1 else MultiHeadAttention2(d_model, d_hidden, num_heads)\n",
    "        self.feed_forward = PosWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask, is_causal=False):\n",
    "        attn_output = self.self_attn(x, x, x, mask=tgt_mask, is_causal=is_causal) # self attention\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # layer-norm + dropout + skip connection\n",
    "        attn_output = self.cross_attn(q=x, k=enc_output, v=enc_output, mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output)) # layer-norm + droput + skip connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output)) # layer-norm + dropout + skip connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, num_heads, d_ff,src_vocab_size, tgt_vocab_size, dropout=0, n_enc_layers=6, n_dec_layers=6, model_kind=1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model=d_model,\n",
    "                         d_hidden=d_hidden,\n",
    "                         num_heads=num_heads,\n",
    "                         d_ff=d_ff,\n",
    "                         model_kind=model_kind) for _ in range(n_enc_layers)]\n",
    "        )\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model=d_model,\n",
    "                         d_hidden=d_hidden,\n",
    "                         d_ff=d_ff,\n",
    "                         num_heads=num_heads,\n",
    "                         model_kind=model_kind) for _ in range(n_dec_layers)]\n",
    "        )\n",
    "\n",
    "        self.pos_encoding_layer = PositionEncoding2(d_embed=d_model)\n",
    "\n",
    "        self.src_embedding = nn.Embedding(embedding_dim=d_model, num_embeddings=src_vocab_size, padding_idx=PAD_IDX)\n",
    "        self.tgt_embedding = nn.Embedding(embedding_dim=d_model, num_embeddings=tgt_vocab_size, padding_idx=PAD_IDX)\n",
    "\n",
    "        self.final = nn.Linear(in_features=d_hidden, out_features=tgt_vocab_size)\n",
    "\n",
    "    def forward(self, inp_seq, tgt_seq, inp_mask=None, tgt_mask=None, is_causal_tgt=False):\n",
    "        \n",
    "\n",
    "        inp_seq = self.src_embedding(inp_seq)\n",
    "        inp_seq = self.pos_encoding_layer(inp_seq)\n",
    "\n",
    "        for enc in self.enc_layers:\n",
    "            inp_seq = enc(inp_seq, mask=inp_mask, is_causal=False)\n",
    "\n",
    "        # inp_seq = self.enc_layers(inp_seq, mask=inp_mask, is_causal=False)\n",
    "\n",
    "        tgt_seq = self.src_embedding(tgt_seq)\n",
    "        # tgt_seq = self.dec_layers(x=tgt_seq, enc_out=inp_seq, mask=tgt_mask, is_causal=is_causal_tgt)\n",
    "        for dec in self.dec_layers:\n",
    "            tgt_seq = dec(x=tgt_seq, enc_output=inp_seq, src_mask=inp_mask, tgt_mask=tgt_mask, is_causal=is_causal_tgt)\n",
    "\n",
    "        out_logits = self.final(tgt_seq)\n",
    "        return out_logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[[ 0.2983,  0.2131,  1.3389, -0.1188, -0.1022,  0.1562, -0.5975],\n",
       "          [ 0.5211, -0.2773,  1.0763, -0.4771, -0.0267,  0.5911, -0.0099],\n",
       "          [ 0.7991,  0.2512,  1.3321,  0.6017, -0.5192,  0.3061, -0.5167]],\n",
       " \n",
       "         [[ 0.8551,  0.0807,  0.3819, -0.2371, -0.1956,  0.2857, -0.5817],\n",
       "          [ 1.3447, -0.0757, -0.2926,  0.3488, -0.6398,  0.2266, -0.4109],\n",
       "          [ 0.5988,  0.6159,  1.3154,  0.3818, -0.5002,  0.1380, -0.8236]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Sample running data through transformer\"\"\"\n",
    "t = Transformer(d_model=8, d_hidden=8, num_heads=4, d_ff=10, src_vocab_size=3, tgt_vocab_size=7)\n",
    "src, tgt = torch.arange(0, 3).tile(2, 1), torch.arange(0, 3).tile(2, 1)\n",
    "\n",
    "src.shape, t(src, tgt)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # print(batch)\n",
    "    src_batch, tgt_batch = list(zip(*batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX).transpose(-1, -2)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX).transpose(-1, -2)\n",
    "\n",
    "    return src_batch, tgt_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   2,  313,  400,  826, 1276,    3]),\n",
       " tensor([   2, 1859, 2613, 5024, 3934,    3]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple task to overfit the model on; helps to catch dumb bugs in the architecture and training\n",
    "text_transform_en('one two three four'), text_transform_en('five six seven eight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "D_MODEL = 512\n",
    "D_FF = 2048\n",
    "N_HEADS = 8\n",
    "\n",
    "BATCH_SIZE = 32#64\n",
    "LR = 0.00001 # make this a schedule\n",
    "BETAS = (0.9, 0.98)\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, data_loader=None):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    tokenized_train_iter = torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), train_iter))\n",
    "    \n",
    "    train_dataloader = DataLoader(tokenized_train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    data_count = 0\n",
    "    import tqdm\n",
    "    for src, tgt in tqdm.tqdm(train_dataloader):#data_loader):#train_dataloader):\n",
    "        data_count+=1\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        out_logits = model(src, tgt[:, :-1], is_causal_tgt=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(out_logits.transpose(-1, -2), tgt[:, 1:])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    print(\"Trained on %s datapts\"%data_count)\n",
    "\n",
    "    return losses / data_count\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    tokenized_test_iter = torch.utils.data.datapipes.iter.sharding.ShardingFilterIterDataPipe(map(lambda x : (text_transform_de(x[0]), text_transform_en(x[1])), test_iter))\n",
    "    valid_dataloader = DataLoader(tokenized_test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    data_count=0\n",
    "    for src, tgt in valid_dataloader:\n",
    "        data_count+=1\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        output = model(src, tgt[:,:-1])\n",
    "\n",
    "    \n",
    "        loss = loss_fn(output.transpose(-1, -2), tgt[:, 1:])\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Transformer(d_model=D_MODEL, \n",
    "                    d_hidden=D_MODEL, \n",
    "                    num_heads=N_HEADS, \n",
    "                    d_ff=D_FF, \n",
    "                    src_vocab_size=de_tokenizer.get_vocab_size(), \n",
    "                    tgt_vocab_size=en_tokenizer.get_vocab_size(),\n",
    "                    model_kind=1 # type 2 is more common (refers to using MultiHeadAttenion2 class in this notebook); see both implementations above for detail\n",
    "                    )\n",
    "\n",
    "optimizer = torch.optim.Adam(betas=BETAS, eps=10e-9, lr=LR, params=model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "907it [10:08,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained on 907 datapts\n",
      "Epoch: 1, Train loss: 5.419, Val loss: 6.631, Epoch time = 608.327s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1#0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)#, train_dataloader)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference \n",
    "y_toks = [BOS_IDX]\n",
    "a_sample = 80\n",
    "src_toks = de_tokenizer.encode(de_data_test[a_sample]).ids\n",
    "model.eval()\n",
    "toks_generated=0\n",
    "max_len=100\n",
    "while y_toks[-1] != EOS_IDX and toks_generated < max_len:\n",
    "    toks_generated+=1\n",
    "    out_logits = model(torch.tensor(src_toks).unsqueeze(0), torch.tensor(y_toks).unsqueeze(0), is_causal_tgt=True)\n",
    "    idx_predicted = torch.argmax(out_logits[:,-1])\n",
    "    y_toks.append(idx_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted tokens:  [2, tensor(218), tensor(384), tensor(559), tensor(201), tensor(287), tensor(170), tensor(97), tensor(487), tensor(287), tensor(170), tensor(97), tensor(487), tensor(149), tensor(3)]\n",
      "text:  Two large dogs are playing on a beach playing on a beach .\n",
      "ground truth:  Two large tan dogs play along a sandy beach.\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted tokens: \", y_toks)\n",
    "print(\"text: \", en_tokenizer.decode(y_toks))\n",
    "print(\"ground truth: \", en_data_test[a_sample])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
